{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd803c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f195fdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>PAQ605</th>\n",
       "      <th>BMXBMI</th>\n",
       "      <th>LBXGLU</th>\n",
       "      <th>DIQ010</th>\n",
       "      <th>LBXGLT</th>\n",
       "      <th>LBXIN</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73564.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>14.91</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73568.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73576.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>6.14</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73577.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>16.15</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73580.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>10.92</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>83711.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>6.53</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>83712.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>13.02</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>83713.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.7</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>21.41</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>83718.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>4.99</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>83727.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>3.76</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1966 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SEQN  RIAGENDR  PAQ605  BMXBMI  LBXGLU  DIQ010  LBXGLT  LBXIN  \\\n",
       "0     73564.0       2.0     2.0    35.7   110.0     2.0   150.0  14.91   \n",
       "1     73568.0       2.0     2.0    20.3    89.0     2.0    80.0   3.85   \n",
       "2     73576.0       1.0     2.0    23.2    89.0     2.0    68.0   6.14   \n",
       "3     73577.0       1.0     2.0    28.9   104.0     NaN    84.0  16.15   \n",
       "4     73580.0       2.0     1.0    35.9   103.0     2.0    81.0  10.92   \n",
       "...       ...       ...     ...     ...     ...     ...     ...    ...   \n",
       "1961  83711.0       2.0     2.0    33.5   100.0     2.0    73.0   6.53   \n",
       "1962  83712.0       1.0     2.0    30.0    93.0     2.0   208.0  13.02   \n",
       "1963  83713.0       1.0     2.0    23.7   103.0     2.0   124.0  21.41   \n",
       "1964  83718.0       2.0     2.0    27.4    90.0     2.0   108.0   4.99   \n",
       "1965  83727.0       1.0     2.0    24.5     NaN     2.0   108.0   3.76   \n",
       "\n",
       "     age_group  \n",
       "0        Adult  \n",
       "1        Adult  \n",
       "2        Adult  \n",
       "3        Adult  \n",
       "4        Adult  \n",
       "...        ...  \n",
       "1961     Adult  \n",
       "1962     Adult  \n",
       "1963     Adult  \n",
       "1964     Adult  \n",
       "1965     Adult  \n",
       "\n",
       "[1966 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('Train_Data_2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a06a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>PAQ605</th>\n",
       "      <th>BMXBMI</th>\n",
       "      <th>LBXGLU</th>\n",
       "      <th>DIQ010</th>\n",
       "      <th>LBXGLT</th>\n",
       "      <th>LBXIN</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>14.91</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>6.14</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>16.15</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>10.92</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>6.53</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>13.02</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.7</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>21.41</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>4.99</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>3.76</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1966 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RIAGENDR  PAQ605  BMXBMI  LBXGLU  DIQ010  LBXGLT  LBXIN age_group\n",
       "0          2.0     2.0    35.7   110.0     2.0   150.0  14.91     Adult\n",
       "1          2.0     2.0    20.3    89.0     2.0    80.0   3.85     Adult\n",
       "2          1.0     2.0    23.2    89.0     2.0    68.0   6.14     Adult\n",
       "3          1.0     2.0    28.9   104.0     NaN    84.0  16.15     Adult\n",
       "4          2.0     1.0    35.9   103.0     2.0    81.0  10.92     Adult\n",
       "...        ...     ...     ...     ...     ...     ...    ...       ...\n",
       "1961       2.0     2.0    33.5   100.0     2.0    73.0   6.53     Adult\n",
       "1962       1.0     2.0    30.0    93.0     2.0   208.0  13.02     Adult\n",
       "1963       1.0     2.0    23.7   103.0     2.0   124.0  21.41     Adult\n",
       "1964       2.0     2.0    27.4    90.0     2.0   108.0   4.99     Adult\n",
       "1965       1.0     2.0    24.5     NaN     2.0   108.0   3.76     Adult\n",
       "\n",
       "[1966 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop(columns=['SEQN'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175e6c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RIAGENDR      0\n",
       "PAQ605       13\n",
       "BMXBMI       18\n",
       "LBXGLU       13\n",
       "DIQ010       18\n",
       "LBXGLT       11\n",
       "LBXIN         9\n",
       "age_group    14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[['RIAGENDR']] = imputer.fit_transform(df[['RIAGENDR']])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82c5368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DAKSH\\AppData\\Local\\Temp\\ipykernel_34288\\980445591.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['BMXBMI'].fillna(df['BMXBMI'].mean(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RIAGENDR      0\n",
       "PAQ605       13\n",
       "BMXBMI        0\n",
       "LBXGLU       13\n",
       "DIQ010       18\n",
       "LBXGLT       11\n",
       "LBXIN         9\n",
       "age_group    14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['BMXBMI'].fillna(df['BMXBMI'].mean(), inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c0a78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RIAGENDR      0\n",
       "PAQ605       13\n",
       "BMXBMI        0\n",
       "LBXGLU       13\n",
       "DIQ010       18\n",
       "LBXGLT       11\n",
       "LBXIN         9\n",
       "age_group     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "df['age_group'] = ordinal_encoder.fit_transform(df[['age_group']])\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[['age_group']] = imputer.fit_transform(df[['age_group']])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0828b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DAKSH\\AppData\\Local\\Temp\\ipykernel_34288\\1405978833.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['LBXIN'].fillna(df['LBXIN'].mean(), inplace=True)\n",
      "C:\\Users\\DAKSH\\AppData\\Local\\Temp\\ipykernel_34288\\1405978833.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['LBXGLT'].fillna(df['LBXGLT'].mean(), inplace=True)\n",
      "C:\\Users\\DAKSH\\AppData\\Local\\Temp\\ipykernel_34288\\1405978833.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['LBXGLU'].fillna(df['LBXGLU'].mean(), inplace=True)\n",
      "C:\\Users\\DAKSH\\AppData\\Local\\Temp\\ipykernel_34288\\1405978833.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['DIQ010'].fillna(df['DIQ010'].mean(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RIAGENDR      0\n",
       "PAQ605       13\n",
       "BMXBMI        0\n",
       "LBXGLU        0\n",
       "DIQ010        0\n",
       "LBXGLT        0\n",
       "LBXIN         0\n",
       "age_group     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LBXIN'].fillna(df['LBXIN'].mean(), inplace=True)\n",
    "df['LBXGLT'].fillna(df['LBXGLT'].mean(), inplace=True)\n",
    "df['LBXGLU'].fillna(df['LBXGLU'].mean(), inplace=True)\n",
    "df['DIQ010'].fillna(df['DIQ010'].mean(), inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2110243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RIAGENDR     0\n",
       "PAQ605       0\n",
       "BMXBMI       0\n",
       "LBXGLU       0\n",
       "DIQ010       0\n",
       "LBXGLT       0\n",
       "LBXIN        0\n",
       "age_group    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[['PAQ605']] = imputer.fit_transform(df[['PAQ605']])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c843370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DAKSH\\AppData\\Local\\Temp\\ipykernel_34288\\96204063.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['BMXBMI'].fillna(test_df['BMXBMI'].mean(), inplace=True)\n",
      "C:\\Users\\DAKSH\\AppData\\Local\\Temp\\ipykernel_34288\\96204063.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['LBXIN'].fillna(test_df['LBXIN'].mean(), inplace=True)\n",
      "C:\\Users\\DAKSH\\AppData\\Local\\Temp\\ipykernel_34288\\96204063.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['LBXGLT'].fillna(test_df['LBXGLT'].mean(), inplace=True)\n",
      "C:\\Users\\DAKSH\\AppData\\Local\\Temp\\ipykernel_34288\\96204063.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['LBXGLU'].fillna(test_df['LBXGLU'].mean(), inplace=True)\n",
      "C:\\Users\\DAKSH\\AppData\\Local\\Temp\\ipykernel_34288\\96204063.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['DIQ010'].fillna(test_df['DIQ010'].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train_df=df\n",
    "test_df=pd.read_csv('Test_Data_2.csv')\n",
    "\n",
    "test_df = test_df.drop(columns=['SEQN'])\n",
    "test_df[['RIAGENDR']] = imputer.fit_transform(test_df[['RIAGENDR']])\n",
    "test_df['BMXBMI'].fillna(test_df['BMXBMI'].mean(), inplace=True)\n",
    "test_df['LBXIN'].fillna(test_df['LBXIN'].mean(), inplace=True)\n",
    "test_df['LBXGLT'].fillna(test_df['LBXGLT'].mean(), inplace=True)\n",
    "test_df['LBXGLU'].fillna(test_df['LBXGLU'].mean(), inplace=True)\n",
    "test_df['DIQ010'].fillna(test_df['DIQ010'].mean(), inplace=True)\n",
    "test_df[['PAQ605']] = imputer.fit_transform(test_df[['PAQ605']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a41b8b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>PAQ605</th>\n",
       "      <th>BMXBMI</th>\n",
       "      <th>LBXGLU</th>\n",
       "      <th>DIQ010</th>\n",
       "      <th>LBXGLT</th>\n",
       "      <th>LBXIN</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>150.0</td>\n",
       "      <td>14.91</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>6.14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>2.015914</td>\n",
       "      <td>84.0</td>\n",
       "      <td>16.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>81.0</td>\n",
       "      <td>10.92</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>6.53</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>208.0</td>\n",
       "      <td>13.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.7</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>124.0</td>\n",
       "      <td>21.41</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>108.0</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>99.491039</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>108.0</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1966 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RIAGENDR  PAQ605  BMXBMI      LBXGLU    DIQ010  LBXGLT  LBXIN  age_group\n",
       "0          2.0     2.0    35.7  110.000000  2.000000   150.0  14.91        0.0\n",
       "1          2.0     2.0    20.3   89.000000  2.000000    80.0   3.85        0.0\n",
       "2          1.0     2.0    23.2   89.000000  2.000000    68.0   6.14        0.0\n",
       "3          1.0     2.0    28.9  104.000000  2.015914    84.0  16.15        0.0\n",
       "4          2.0     1.0    35.9  103.000000  2.000000    81.0  10.92        0.0\n",
       "...        ...     ...     ...         ...       ...     ...    ...        ...\n",
       "1961       2.0     2.0    33.5  100.000000  2.000000    73.0   6.53        0.0\n",
       "1962       1.0     2.0    30.0   93.000000  2.000000   208.0  13.02        0.0\n",
       "1963       1.0     2.0    23.7  103.000000  2.000000   124.0  21.41        0.0\n",
       "1964       2.0     2.0    27.4   90.000000  2.000000   108.0   4.99        0.0\n",
       "1965       1.0     2.0    24.5   99.491039  2.000000   108.0   3.76        0.0\n",
       "\n",
       "[1966 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd2cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "\n",
    "    model=Sequential()\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(hp.Int('num_layers', 1, 10)):\n",
    "\n",
    "        if counter == 0:\n",
    "            model.add(Dense(\n",
    "                hp.Int('units'+str(i), min_value=8, max_value=128, step=8), \n",
    "                activation=hp.Choice('activation'+str(i), ['relu', 'tanh', 'sigmoid']),\n",
    "                input_dim=7\n",
    "                )\n",
    "            )\n",
    "            model.add(Dropout(hp.Choice('dropout'+str(i), [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])))\n",
    "        else:\n",
    "            model.add(Dense(\n",
    "                hp.Int('units'+str(i), min_value=8, max_value=128, step=8), \n",
    "                activation=hp.Choice('activation'+str(i), ['relu', 'tanh', 'sigmoid'])\n",
    "                )\n",
    "            )\n",
    "            model.add(Dropout(hp.Choice('dropout'+str(i), [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])))\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop', 'adagrad', 'adadelta']),\n",
    "                     loss='binary_crossentropy', \n",
    "                     metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7350b35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\final\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "tuner=kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3,\n",
    "    directory='my_dir',\n",
    "    project_name='final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dc59dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop('age_group', axis=1)\n",
    "y=df['age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69143da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6f4b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea58017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f29930f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 1,\n",
       " 'units0': 8,\n",
       " 'activation0': 'tanh',\n",
       " 'dropout0': 0.4,\n",
       " 'optimizer': 'rmsprop',\n",
       " 'units1': 128,\n",
       " 'activation1': 'relu',\n",
       " 'dropout1': 0.9,\n",
       " 'units2': 80,\n",
       " 'activation2': 'tanh',\n",
       " 'dropout2': 0.1,\n",
       " 'units3': 48,\n",
       " 'activation3': 'tanh',\n",
       " 'dropout3': 0.8,\n",
       " 'units4': 104,\n",
       " 'activation4': 'sigmoid',\n",
       " 'dropout4': 0.2,\n",
       " 'units5': 24,\n",
       " 'activation5': 'relu',\n",
       " 'dropout5': 0.3,\n",
       " 'units6': 8,\n",
       " 'activation6': 'relu',\n",
       " 'dropout6': 0.6,\n",
       " 'units7': 32,\n",
       " 'activation7': 'relu',\n",
       " 'dropout7': 0.4,\n",
       " 'units8': 88,\n",
       " 'activation8': 'relu',\n",
       " 'dropout8': 0.9}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16c9d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b845ddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200\n",
      "50/50 [==============================] - 1s 5ms/step - loss: 0.5004 - accuracy: 0.7964 - val_loss: 0.4461 - val_accuracy: 0.8426\n",
      "Epoch 8/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4928 - accuracy: 0.7945 - val_loss: 0.4261 - val_accuracy: 0.8477\n",
      "Epoch 9/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4671 - accuracy: 0.8034 - val_loss: 0.4123 - val_accuracy: 0.8503\n",
      "Epoch 10/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.8136 - val_loss: 0.4004 - val_accuracy: 0.8503\n",
      "Epoch 11/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4482 - accuracy: 0.8085 - val_loss: 0.3920 - val_accuracy: 0.8503\n",
      "Epoch 12/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8238 - val_loss: 0.3873 - val_accuracy: 0.8528\n",
      "Epoch 13/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4363 - accuracy: 0.8200 - val_loss: 0.3839 - val_accuracy: 0.8528\n",
      "Epoch 14/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4419 - accuracy: 0.8155 - val_loss: 0.3815 - val_accuracy: 0.8503\n",
      "Epoch 15/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4332 - accuracy: 0.8219 - val_loss: 0.3794 - val_accuracy: 0.8503\n",
      "Epoch 16/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4258 - accuracy: 0.8225 - val_loss: 0.3780 - val_accuracy: 0.8528\n",
      "Epoch 17/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.8251 - val_loss: 0.3770 - val_accuracy: 0.8503\n",
      "Epoch 18/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4234 - accuracy: 0.8193 - val_loss: 0.3766 - val_accuracy: 0.8503\n",
      "Epoch 19/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.8212 - val_loss: 0.3756 - val_accuracy: 0.8503\n",
      "Epoch 20/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4122 - accuracy: 0.8225 - val_loss: 0.3748 - val_accuracy: 0.8503\n",
      "Epoch 21/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.8238 - val_loss: 0.3741 - val_accuracy: 0.8528\n",
      "Epoch 22/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4150 - accuracy: 0.8219 - val_loss: 0.3733 - val_accuracy: 0.8528\n",
      "Epoch 23/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4219 - accuracy: 0.8187 - val_loss: 0.3727 - val_accuracy: 0.8528\n",
      "Epoch 24/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4175 - accuracy: 0.8346 - val_loss: 0.3723 - val_accuracy: 0.8528\n",
      "Epoch 25/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4112 - accuracy: 0.8257 - val_loss: 0.3718 - val_accuracy: 0.8528\n",
      "Epoch 26/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4122 - accuracy: 0.8212 - val_loss: 0.3715 - val_accuracy: 0.8528\n",
      "Epoch 27/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4086 - accuracy: 0.8295 - val_loss: 0.3710 - val_accuracy: 0.8553\n",
      "Epoch 28/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4142 - accuracy: 0.8251 - val_loss: 0.3710 - val_accuracy: 0.8553\n",
      "Epoch 29/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4130 - accuracy: 0.8302 - val_loss: 0.3707 - val_accuracy: 0.8553\n",
      "Epoch 30/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4146 - accuracy: 0.8302 - val_loss: 0.3706 - val_accuracy: 0.8553\n",
      "Epoch 31/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8302 - val_loss: 0.3703 - val_accuracy: 0.8553\n",
      "Epoch 32/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4046 - accuracy: 0.8346 - val_loss: 0.3701 - val_accuracy: 0.8528\n",
      "Epoch 33/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4012 - accuracy: 0.8352 - val_loss: 0.3703 - val_accuracy: 0.8528\n",
      "Epoch 34/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4113 - accuracy: 0.8257 - val_loss: 0.3700 - val_accuracy: 0.8528\n",
      "Epoch 35/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4078 - accuracy: 0.8321 - val_loss: 0.3694 - val_accuracy: 0.8528\n",
      "Epoch 36/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4107 - accuracy: 0.8257 - val_loss: 0.3697 - val_accuracy: 0.8528\n",
      "Epoch 37/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4130 - accuracy: 0.8206 - val_loss: 0.3696 - val_accuracy: 0.8528\n",
      "Epoch 38/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4086 - accuracy: 0.8282 - val_loss: 0.3692 - val_accuracy: 0.8528\n",
      "Epoch 39/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4097 - accuracy: 0.8219 - val_loss: 0.3695 - val_accuracy: 0.8553\n",
      "Epoch 40/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4079 - accuracy: 0.8251 - val_loss: 0.3696 - val_accuracy: 0.8579\n",
      "Epoch 41/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8340 - val_loss: 0.3696 - val_accuracy: 0.8604\n",
      "Epoch 42/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8244 - val_loss: 0.3692 - val_accuracy: 0.8579\n",
      "Epoch 43/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4084 - accuracy: 0.8251 - val_loss: 0.3687 - val_accuracy: 0.8553\n",
      "Epoch 44/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4061 - accuracy: 0.8346 - val_loss: 0.3684 - val_accuracy: 0.8553\n",
      "Epoch 45/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4008 - accuracy: 0.8314 - val_loss: 0.3680 - val_accuracy: 0.8553\n",
      "Epoch 46/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4097 - accuracy: 0.8276 - val_loss: 0.3683 - val_accuracy: 0.8553\n",
      "Epoch 47/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4004 - accuracy: 0.8276 - val_loss: 0.3681 - val_accuracy: 0.8553\n",
      "Epoch 48/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8206 - val_loss: 0.3678 - val_accuracy: 0.8553\n",
      "Epoch 49/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.8232 - val_loss: 0.3676 - val_accuracy: 0.8553\n",
      "Epoch 50/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4098 - accuracy: 0.8238 - val_loss: 0.3671 - val_accuracy: 0.8553\n",
      "Epoch 51/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3980 - accuracy: 0.8314 - val_loss: 0.3672 - val_accuracy: 0.8604\n",
      "Epoch 52/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3961 - accuracy: 0.8340 - val_loss: 0.3670 - val_accuracy: 0.8604\n",
      "Epoch 53/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4041 - accuracy: 0.8270 - val_loss: 0.3672 - val_accuracy: 0.8604\n",
      "Epoch 54/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4044 - accuracy: 0.8276 - val_loss: 0.3669 - val_accuracy: 0.8604\n",
      "Epoch 55/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3994 - accuracy: 0.8314 - val_loss: 0.3668 - val_accuracy: 0.8604\n",
      "Epoch 56/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4025 - accuracy: 0.8282 - val_loss: 0.3666 - val_accuracy: 0.8629\n",
      "Epoch 57/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.8302 - val_loss: 0.3667 - val_accuracy: 0.8604\n",
      "Epoch 58/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4059 - accuracy: 0.8238 - val_loss: 0.3670 - val_accuracy: 0.8604\n",
      "Epoch 59/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3965 - accuracy: 0.8289 - val_loss: 0.3671 - val_accuracy: 0.8604\n",
      "Epoch 60/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8282 - val_loss: 0.3667 - val_accuracy: 0.8604\n",
      "Epoch 61/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3932 - accuracy: 0.8333 - val_loss: 0.3665 - val_accuracy: 0.8604\n",
      "Epoch 62/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4013 - accuracy: 0.8270 - val_loss: 0.3664 - val_accuracy: 0.8629\n",
      "Epoch 63/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8295 - val_loss: 0.3663 - val_accuracy: 0.8629\n",
      "Epoch 64/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4040 - accuracy: 0.8270 - val_loss: 0.3663 - val_accuracy: 0.8629\n",
      "Epoch 65/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3991 - accuracy: 0.8308 - val_loss: 0.3659 - val_accuracy: 0.8629\n",
      "Epoch 66/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3946 - accuracy: 0.8270 - val_loss: 0.3658 - val_accuracy: 0.8604\n",
      "Epoch 67/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3986 - accuracy: 0.8232 - val_loss: 0.3658 - val_accuracy: 0.8604\n",
      "Epoch 68/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4028 - accuracy: 0.8244 - val_loss: 0.3659 - val_accuracy: 0.8629\n",
      "Epoch 69/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8302 - val_loss: 0.3656 - val_accuracy: 0.8604\n",
      "Epoch 70/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.8295 - val_loss: 0.3656 - val_accuracy: 0.8604\n",
      "Epoch 71/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3969 - accuracy: 0.8289 - val_loss: 0.3654 - val_accuracy: 0.8604\n",
      "Epoch 72/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4020 - accuracy: 0.8289 - val_loss: 0.3657 - val_accuracy: 0.8604\n",
      "Epoch 73/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3963 - accuracy: 0.8295 - val_loss: 0.3660 - val_accuracy: 0.8629\n",
      "Epoch 74/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4018 - accuracy: 0.8219 - val_loss: 0.3659 - val_accuracy: 0.8629\n",
      "Epoch 75/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3981 - accuracy: 0.8276 - val_loss: 0.3659 - val_accuracy: 0.8604\n",
      "Epoch 76/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4000 - accuracy: 0.8244 - val_loss: 0.3661 - val_accuracy: 0.8629\n",
      "Epoch 77/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3927 - accuracy: 0.8289 - val_loss: 0.3659 - val_accuracy: 0.8629\n",
      "Epoch 78/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8295 - val_loss: 0.3660 - val_accuracy: 0.8629\n",
      "Epoch 79/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4015 - accuracy: 0.8257 - val_loss: 0.3660 - val_accuracy: 0.8579\n",
      "Epoch 80/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.8302 - val_loss: 0.3658 - val_accuracy: 0.8579\n",
      "Epoch 81/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3992 - accuracy: 0.8257 - val_loss: 0.3656 - val_accuracy: 0.8553\n",
      "Epoch 82/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3949 - accuracy: 0.8327 - val_loss: 0.3657 - val_accuracy: 0.8553\n",
      "Epoch 83/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3963 - accuracy: 0.8219 - val_loss: 0.3654 - val_accuracy: 0.8553\n",
      "Epoch 84/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3948 - accuracy: 0.8270 - val_loss: 0.3657 - val_accuracy: 0.8553\n",
      "Epoch 85/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4001 - accuracy: 0.8289 - val_loss: 0.3657 - val_accuracy: 0.8553\n",
      "Epoch 86/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4002 - accuracy: 0.8327 - val_loss: 0.3661 - val_accuracy: 0.8553\n",
      "Epoch 87/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.8257 - val_loss: 0.3661 - val_accuracy: 0.8553\n",
      "Epoch 88/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8270 - val_loss: 0.3658 - val_accuracy: 0.8553\n",
      "Epoch 89/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3962 - accuracy: 0.8263 - val_loss: 0.3657 - val_accuracy: 0.8604\n",
      "Epoch 90/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3999 - accuracy: 0.8282 - val_loss: 0.3659 - val_accuracy: 0.8579\n",
      "Epoch 91/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4013 - accuracy: 0.8200 - val_loss: 0.3658 - val_accuracy: 0.8553\n",
      "Epoch 92/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8225 - val_loss: 0.3662 - val_accuracy: 0.8553\n",
      "Epoch 93/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3949 - accuracy: 0.8251 - val_loss: 0.3664 - val_accuracy: 0.8604\n",
      "Epoch 94/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3992 - accuracy: 0.8257 - val_loss: 0.3664 - val_accuracy: 0.8604\n",
      "Epoch 95/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8257 - val_loss: 0.3665 - val_accuracy: 0.8604\n",
      "Epoch 96/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3987 - accuracy: 0.8289 - val_loss: 0.3672 - val_accuracy: 0.8604\n",
      "Epoch 97/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3933 - accuracy: 0.8276 - val_loss: 0.3668 - val_accuracy: 0.8604\n",
      "Epoch 98/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8289 - val_loss: 0.3665 - val_accuracy: 0.8579\n",
      "Epoch 99/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3911 - accuracy: 0.8289 - val_loss: 0.3661 - val_accuracy: 0.8604\n",
      "Epoch 100/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8270 - val_loss: 0.3663 - val_accuracy: 0.8604\n",
      "Epoch 101/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3949 - accuracy: 0.8251 - val_loss: 0.3662 - val_accuracy: 0.8579\n",
      "Epoch 102/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3970 - accuracy: 0.8321 - val_loss: 0.3665 - val_accuracy: 0.8604\n",
      "Epoch 103/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.8263 - val_loss: 0.3665 - val_accuracy: 0.8604\n",
      "Epoch 104/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3973 - accuracy: 0.8263 - val_loss: 0.3665 - val_accuracy: 0.8579\n",
      "Epoch 105/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3980 - accuracy: 0.8282 - val_loss: 0.3665 - val_accuracy: 0.8604\n",
      "Epoch 106/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3854 - accuracy: 0.8276 - val_loss: 0.3662 - val_accuracy: 0.8604\n",
      "Epoch 107/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8257 - val_loss: 0.3662 - val_accuracy: 0.8604\n",
      "Epoch 108/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3964 - accuracy: 0.8270 - val_loss: 0.3664 - val_accuracy: 0.8604\n",
      "Epoch 109/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3921 - accuracy: 0.8282 - val_loss: 0.3664 - val_accuracy: 0.8604\n",
      "Epoch 110/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3956 - accuracy: 0.8270 - val_loss: 0.3663 - val_accuracy: 0.8604\n",
      "Epoch 111/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3982 - accuracy: 0.8257 - val_loss: 0.3660 - val_accuracy: 0.8553\n",
      "Epoch 112/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8257 - val_loss: 0.3661 - val_accuracy: 0.8579\n",
      "Epoch 113/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3963 - accuracy: 0.8251 - val_loss: 0.3659 - val_accuracy: 0.8579\n",
      "Epoch 114/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3957 - accuracy: 0.8257 - val_loss: 0.3661 - val_accuracy: 0.8604\n",
      "Epoch 115/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3982 - accuracy: 0.8263 - val_loss: 0.3659 - val_accuracy: 0.8579\n",
      "Epoch 116/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3956 - accuracy: 0.8295 - val_loss: 0.3659 - val_accuracy: 0.8553\n",
      "Epoch 117/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3899 - accuracy: 0.8340 - val_loss: 0.3663 - val_accuracy: 0.8604\n",
      "Epoch 118/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8270 - val_loss: 0.3660 - val_accuracy: 0.8604\n",
      "Epoch 119/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3950 - accuracy: 0.8212 - val_loss: 0.3661 - val_accuracy: 0.8604\n",
      "Epoch 120/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3988 - accuracy: 0.8251 - val_loss: 0.3662 - val_accuracy: 0.8604\n",
      "Epoch 121/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3950 - accuracy: 0.8308 - val_loss: 0.3660 - val_accuracy: 0.8553\n",
      "Epoch 122/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3975 - accuracy: 0.8225 - val_loss: 0.3660 - val_accuracy: 0.8553\n",
      "Epoch 123/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3959 - accuracy: 0.8276 - val_loss: 0.3663 - val_accuracy: 0.8553\n",
      "Epoch 124/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.8346 - val_loss: 0.3662 - val_accuracy: 0.8553\n",
      "Epoch 125/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3950 - accuracy: 0.8295 - val_loss: 0.3660 - val_accuracy: 0.8553\n",
      "Epoch 126/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3922 - accuracy: 0.8276 - val_loss: 0.3657 - val_accuracy: 0.8579\n",
      "Epoch 127/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3988 - accuracy: 0.8282 - val_loss: 0.3658 - val_accuracy: 0.8579\n",
      "Epoch 128/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3988 - accuracy: 0.8263 - val_loss: 0.3661 - val_accuracy: 0.8579\n",
      "Epoch 129/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3903 - accuracy: 0.8365 - val_loss: 0.3661 - val_accuracy: 0.8604\n",
      "Epoch 130/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3986 - accuracy: 0.8295 - val_loss: 0.3660 - val_accuracy: 0.8604\n",
      "Epoch 131/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8346 - val_loss: 0.3661 - val_accuracy: 0.8604\n",
      "Epoch 132/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3879 - accuracy: 0.8352 - val_loss: 0.3658 - val_accuracy: 0.8579\n",
      "Epoch 133/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3955 - accuracy: 0.8314 - val_loss: 0.3660 - val_accuracy: 0.8604\n",
      "Epoch 134/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3960 - accuracy: 0.8251 - val_loss: 0.3662 - val_accuracy: 0.8604\n",
      "Epoch 135/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3946 - accuracy: 0.8295 - val_loss: 0.3661 - val_accuracy: 0.8579\n",
      "Epoch 136/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8282 - val_loss: 0.3663 - val_accuracy: 0.8579\n",
      "Epoch 137/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.8232 - val_loss: 0.3663 - val_accuracy: 0.8553\n",
      "Epoch 138/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3985 - accuracy: 0.8232 - val_loss: 0.3668 - val_accuracy: 0.8579\n",
      "Epoch 139/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3924 - accuracy: 0.8340 - val_loss: 0.3669 - val_accuracy: 0.8629\n",
      "Epoch 140/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8257 - val_loss: 0.3666 - val_accuracy: 0.8629\n",
      "Epoch 141/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3909 - accuracy: 0.8295 - val_loss: 0.3665 - val_accuracy: 0.8629\n",
      "Epoch 142/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8263 - val_loss: 0.3666 - val_accuracy: 0.8629\n",
      "Epoch 143/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8308 - val_loss: 0.3666 - val_accuracy: 0.8629\n",
      "Epoch 144/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3869 - accuracy: 0.8333 - val_loss: 0.3670 - val_accuracy: 0.8655\n",
      "Epoch 145/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3909 - accuracy: 0.8314 - val_loss: 0.3671 - val_accuracy: 0.8655\n",
      "Epoch 146/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3947 - accuracy: 0.8263 - val_loss: 0.3672 - val_accuracy: 0.8604\n",
      "Epoch 147/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3920 - accuracy: 0.8302 - val_loss: 0.3672 - val_accuracy: 0.8655\n",
      "Epoch 148/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3977 - accuracy: 0.8282 - val_loss: 0.3671 - val_accuracy: 0.8655\n",
      "Epoch 149/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3911 - accuracy: 0.8289 - val_loss: 0.3667 - val_accuracy: 0.8680\n",
      "Epoch 150/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8263 - val_loss: 0.3665 - val_accuracy: 0.8629\n",
      "Epoch 151/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3976 - accuracy: 0.8263 - val_loss: 0.3664 - val_accuracy: 0.8629\n",
      "Epoch 152/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3921 - accuracy: 0.8270 - val_loss: 0.3664 - val_accuracy: 0.8604\n",
      "Epoch 153/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3940 - accuracy: 0.8251 - val_loss: 0.3668 - val_accuracy: 0.8629\n",
      "Epoch 154/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8270 - val_loss: 0.3669 - val_accuracy: 0.8629\n",
      "Epoch 155/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8282 - val_loss: 0.3671 - val_accuracy: 0.8629\n",
      "Epoch 156/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3934 - accuracy: 0.8244 - val_loss: 0.3670 - val_accuracy: 0.8604\n",
      "Epoch 157/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3867 - accuracy: 0.8378 - val_loss: 0.3670 - val_accuracy: 0.8629\n",
      "Epoch 158/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8276 - val_loss: 0.3673 - val_accuracy: 0.8629\n",
      "Epoch 159/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3946 - accuracy: 0.8263 - val_loss: 0.3673 - val_accuracy: 0.8629\n",
      "Epoch 160/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3887 - accuracy: 0.8346 - val_loss: 0.3670 - val_accuracy: 0.8680\n",
      "Epoch 161/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8219 - val_loss: 0.3668 - val_accuracy: 0.8680\n",
      "Epoch 162/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3970 - accuracy: 0.8219 - val_loss: 0.3666 - val_accuracy: 0.8680\n",
      "Epoch 163/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8238 - val_loss: 0.3669 - val_accuracy: 0.8680\n",
      "Epoch 164/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3921 - accuracy: 0.8257 - val_loss: 0.3667 - val_accuracy: 0.8680\n",
      "Epoch 165/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8251 - val_loss: 0.3667 - val_accuracy: 0.8680\n",
      "Epoch 166/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3977 - accuracy: 0.8238 - val_loss: 0.3667 - val_accuracy: 0.8680\n",
      "Epoch 167/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3928 - accuracy: 0.8270 - val_loss: 0.3663 - val_accuracy: 0.8680\n",
      "Epoch 168/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8232 - val_loss: 0.3666 - val_accuracy: 0.8680\n",
      "Epoch 169/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3865 - accuracy: 0.8282 - val_loss: 0.3668 - val_accuracy: 0.8680\n",
      "Epoch 170/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3928 - accuracy: 0.8193 - val_loss: 0.3668 - val_accuracy: 0.8680\n",
      "Epoch 171/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8263 - val_loss: 0.3666 - val_accuracy: 0.8629\n",
      "Epoch 172/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3965 - accuracy: 0.8282 - val_loss: 0.3665 - val_accuracy: 0.8629\n",
      "Epoch 173/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3950 - accuracy: 0.8251 - val_loss: 0.3664 - val_accuracy: 0.8629\n",
      "Epoch 174/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3994 - accuracy: 0.8232 - val_loss: 0.3665 - val_accuracy: 0.8604\n",
      "Epoch 175/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8346 - val_loss: 0.3667 - val_accuracy: 0.8629\n",
      "Epoch 176/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8321 - val_loss: 0.3664 - val_accuracy: 0.8629\n",
      "Epoch 177/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3852 - accuracy: 0.8327 - val_loss: 0.3665 - val_accuracy: 0.8629\n",
      "Epoch 178/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3915 - accuracy: 0.8257 - val_loss: 0.3667 - val_accuracy: 0.8680\n",
      "Epoch 179/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3900 - accuracy: 0.8282 - val_loss: 0.3668 - val_accuracy: 0.8680\n",
      "Epoch 180/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8251 - val_loss: 0.3668 - val_accuracy: 0.8680\n",
      "Epoch 181/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3900 - accuracy: 0.8321 - val_loss: 0.3670 - val_accuracy: 0.8680\n",
      "Epoch 182/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8276 - val_loss: 0.3666 - val_accuracy: 0.8629\n",
      "Epoch 183/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3989 - accuracy: 0.8257 - val_loss: 0.3666 - val_accuracy: 0.8629\n",
      "Epoch 184/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3971 - accuracy: 0.8276 - val_loss: 0.3664 - val_accuracy: 0.8604\n",
      "Epoch 185/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8282 - val_loss: 0.3663 - val_accuracy: 0.8629\n",
      "Epoch 186/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3962 - accuracy: 0.8212 - val_loss: 0.3663 - val_accuracy: 0.8604\n",
      "Epoch 187/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3942 - accuracy: 0.8282 - val_loss: 0.3662 - val_accuracy: 0.8604\n",
      "Epoch 188/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3952 - accuracy: 0.8276 - val_loss: 0.3662 - val_accuracy: 0.8604\n",
      "Epoch 189/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3884 - accuracy: 0.8308 - val_loss: 0.3661 - val_accuracy: 0.8629\n",
      "Epoch 190/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8333 - val_loss: 0.3665 - val_accuracy: 0.8629\n",
      "Epoch 191/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3981 - accuracy: 0.8219 - val_loss: 0.3666 - val_accuracy: 0.8629\n",
      "Epoch 192/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8276 - val_loss: 0.3664 - val_accuracy: 0.8604\n",
      "Epoch 193/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3915 - accuracy: 0.8263 - val_loss: 0.3663 - val_accuracy: 0.8629\n",
      "Epoch 194/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3880 - accuracy: 0.8352 - val_loss: 0.3664 - val_accuracy: 0.8680\n",
      "Epoch 195/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8321 - val_loss: 0.3665 - val_accuracy: 0.8629\n",
      "Epoch 196/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8263 - val_loss: 0.3666 - val_accuracy: 0.8680\n",
      "Epoch 197/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3932 - accuracy: 0.8212 - val_loss: 0.3663 - val_accuracy: 0.8680\n",
      "Epoch 198/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3941 - accuracy: 0.8232 - val_loss: 0.3660 - val_accuracy: 0.8629\n",
      "Epoch 199/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3982 - accuracy: 0.8232 - val_loss: 0.3659 - val_accuracy: 0.8629\n",
      "Epoch 200/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8352 - val_loss: 0.3659 - val_accuracy: 0.8629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2f809d19b40>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=200, initial_epoch=6, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4140ea2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>PAQ605</th>\n",
       "      <th>BMXBMI</th>\n",
       "      <th>LBXGLU</th>\n",
       "      <th>DIQ010</th>\n",
       "      <th>LBXGLT</th>\n",
       "      <th>LBXIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.2</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>15.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>15.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.6</td>\n",
       "      <td>107.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>8.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>12.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.7</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>3.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.9</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.3</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>6.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>41.5</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>15.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>8.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     RIAGENDR  PAQ605  BMXBMI  LBXGLU  DIQ010  LBXGLT  LBXIN\n",
       "0         1.0     1.0    32.2    96.0     2.0   135.0  15.11\n",
       "1         2.0     2.0    26.3   100.0     2.0   141.0  15.26\n",
       "2         1.0     2.0    28.6   107.0     2.0   136.0   8.82\n",
       "3         2.0     1.0    22.1    93.0     2.0   111.0  12.13\n",
       "4         1.0     1.0    24.7    91.0     2.0   105.0   3.12\n",
       "..        ...     ...     ...     ...     ...     ...    ...\n",
       "307       2.0     2.0    21.9    82.0     2.0    82.0   2.54\n",
       "308       2.0     1.0    33.3    95.0     2.0    77.0   6.36\n",
       "309       2.0     2.0    41.5    91.0     2.0   149.0  15.52\n",
       "310       2.0     2.0    22.5    82.0     2.0    93.0   1.39\n",
       "311       1.0     2.0    16.1    96.0     2.0   197.0   8.57\n",
       "\n",
       "[312 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69d4ad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test_x = test_df\n",
    "test_x = scaler.transform(test_x)\n",
    "predictions = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1908703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06268957],\n",
       "       [0.18849273],\n",
       "       [0.33997035],\n",
       "       [0.04919891],\n",
       "       [0.0664616 ],\n",
       "       [0.21195932],\n",
       "       [0.23991555],\n",
       "       [0.35867208],\n",
       "       [0.05000138],\n",
       "       [0.11594177],\n",
       "       [0.26818076],\n",
       "       [0.26319838],\n",
       "       [0.06639483],\n",
       "       [0.25021222],\n",
       "       [0.4712879 ],\n",
       "       [0.11528715],\n",
       "       [0.38807058],\n",
       "       [0.2583438 ],\n",
       "       [0.07363681],\n",
       "       [0.09312213],\n",
       "       [0.45529163],\n",
       "       [0.0608523 ],\n",
       "       [0.17424314],\n",
       "       [0.12356527],\n",
       "       [0.06243189],\n",
       "       [0.13243641],\n",
       "       [0.03639898],\n",
       "       [0.12069231],\n",
       "       [0.21840346],\n",
       "       [0.06280608],\n",
       "       [0.05015732],\n",
       "       [0.2529818 ],\n",
       "       [0.1662897 ],\n",
       "       [0.4796309 ],\n",
       "       [0.21317096],\n",
       "       [0.05443613],\n",
       "       [0.0420496 ],\n",
       "       [0.27853134],\n",
       "       [0.05770534],\n",
       "       [0.08467328],\n",
       "       [0.08934253],\n",
       "       [0.06213131],\n",
       "       [0.18902797],\n",
       "       [0.07719868],\n",
       "       [0.08593103],\n",
       "       [0.1053642 ],\n",
       "       [0.08545961],\n",
       "       [0.27343917],\n",
       "       [0.07085396],\n",
       "       [0.4268216 ],\n",
       "       [0.5371526 ],\n",
       "       [0.13931617],\n",
       "       [0.07439702],\n",
       "       [0.42407945],\n",
       "       [0.28735268],\n",
       "       [0.259562  ],\n",
       "       [0.1442446 ],\n",
       "       [0.33195972],\n",
       "       [0.40284398],\n",
       "       [0.40609524],\n",
       "       [0.06225809],\n",
       "       [0.12917131],\n",
       "       [0.08882299],\n",
       "       [0.04203543],\n",
       "       [0.25153148],\n",
       "       [0.04449846],\n",
       "       [0.069003  ],\n",
       "       [0.49697477],\n",
       "       [0.06058094],\n",
       "       [0.2400806 ],\n",
       "       [0.08072358],\n",
       "       [0.08957694],\n",
       "       [0.05168518],\n",
       "       [0.0434908 ],\n",
       "       [0.06308962],\n",
       "       [0.0810887 ],\n",
       "       [0.16757047],\n",
       "       [0.13787663],\n",
       "       [0.12682731],\n",
       "       [0.06901133],\n",
       "       [0.07748162],\n",
       "       [0.05353082],\n",
       "       [0.08479699],\n",
       "       [0.04933975],\n",
       "       [0.1137532 ],\n",
       "       [0.12841347],\n",
       "       [0.21182892],\n",
       "       [0.16439945],\n",
       "       [0.06701542],\n",
       "       [0.16170079],\n",
       "       [0.37450334],\n",
       "       [0.21082222],\n",
       "       [0.06147117],\n",
       "       [0.10000888],\n",
       "       [0.06614054],\n",
       "       [0.48337418],\n",
       "       [0.4323596 ],\n",
       "       [0.0531844 ],\n",
       "       [0.19605283],\n",
       "       [0.11601797],\n",
       "       [0.2595736 ],\n",
       "       [0.2113168 ],\n",
       "       [0.22594306],\n",
       "       [0.11226313],\n",
       "       [0.04097759],\n",
       "       [0.06930535],\n",
       "       [0.07151223],\n",
       "       [0.13402109],\n",
       "       [0.33954537],\n",
       "       [0.04890747],\n",
       "       [0.05729613],\n",
       "       [0.07819242],\n",
       "       [0.03940144],\n",
       "       [0.0583961 ],\n",
       "       [0.04166248],\n",
       "       [0.07706504],\n",
       "       [0.05643723],\n",
       "       [0.08220704],\n",
       "       [0.17978989],\n",
       "       [0.18434972],\n",
       "       [0.03985021],\n",
       "       [0.06343104],\n",
       "       [0.2771039 ],\n",
       "       [0.12507796],\n",
       "       [0.07719842],\n",
       "       [0.0398537 ],\n",
       "       [0.05639592],\n",
       "       [0.23920104],\n",
       "       [0.19209807],\n",
       "       [0.13386668],\n",
       "       [0.14570157],\n",
       "       [0.03565303],\n",
       "       [0.39753714],\n",
       "       [0.04576959],\n",
       "       [0.11226601],\n",
       "       [0.04692815],\n",
       "       [0.18143599],\n",
       "       [0.5188611 ],\n",
       "       [0.09336182],\n",
       "       [0.11234282],\n",
       "       [0.423623  ],\n",
       "       [0.47929436],\n",
       "       [0.04676145],\n",
       "       [0.21994802],\n",
       "       [0.30938396],\n",
       "       [0.13028723],\n",
       "       [0.07197481],\n",
       "       [0.07759939],\n",
       "       [0.09526337],\n",
       "       [0.28675658],\n",
       "       [0.24350418],\n",
       "       [0.12264662],\n",
       "       [0.08071087],\n",
       "       [0.07265498],\n",
       "       [0.13255289],\n",
       "       [0.24885678],\n",
       "       [0.10001733],\n",
       "       [0.04525711],\n",
       "       [0.11848985],\n",
       "       [0.08690245],\n",
       "       [0.06050668],\n",
       "       [0.54378176],\n",
       "       [0.44363514],\n",
       "       [0.4312868 ],\n",
       "       [0.1393473 ],\n",
       "       [0.21106228],\n",
       "       [0.24417157],\n",
       "       [0.23779649],\n",
       "       [0.09428049],\n",
       "       [0.10890161],\n",
       "       [0.04780636],\n",
       "       [0.08298986],\n",
       "       [0.5339272 ],\n",
       "       [0.2736668 ],\n",
       "       [0.22985762],\n",
       "       [0.03776874],\n",
       "       [0.17084523],\n",
       "       [0.06241452],\n",
       "       [0.35468408],\n",
       "       [0.15703906],\n",
       "       [0.19947575],\n",
       "       [0.06160098],\n",
       "       [0.1303324 ],\n",
       "       [0.41147703],\n",
       "       [0.18485978],\n",
       "       [0.09480133],\n",
       "       [0.1042302 ],\n",
       "       [0.3062387 ],\n",
       "       [0.12012649],\n",
       "       [0.03683671],\n",
       "       [0.1312106 ],\n",
       "       [0.07696506],\n",
       "       [0.0655421 ],\n",
       "       [0.1054226 ],\n",
       "       [0.1556259 ],\n",
       "       [0.16659155],\n",
       "       [0.07084667],\n",
       "       [0.1295483 ],\n",
       "       [0.05863936],\n",
       "       [0.33324754],\n",
       "       [0.07801085],\n",
       "       [0.08030321],\n",
       "       [0.06267877],\n",
       "       [0.05327642],\n",
       "       [0.0578393 ],\n",
       "       [0.0630099 ],\n",
       "       [0.07723223],\n",
       "       [0.10671127],\n",
       "       [0.04818731],\n",
       "       [0.17897926],\n",
       "       [0.06158139],\n",
       "       [0.12516366],\n",
       "       [0.10063828],\n",
       "       [0.06089365],\n",
       "       [0.06279774],\n",
       "       [0.03718344],\n",
       "       [0.07727856],\n",
       "       [0.2790529 ],\n",
       "       [0.0543895 ],\n",
       "       [0.24732573],\n",
       "       [0.0452273 ],\n",
       "       [0.11760698],\n",
       "       [0.27603495],\n",
       "       [0.05652968],\n",
       "       [0.06779157],\n",
       "       [0.12089796],\n",
       "       [0.21613675],\n",
       "       [0.08488009],\n",
       "       [0.07611447],\n",
       "       [0.19510172],\n",
       "       [0.10637366],\n",
       "       [0.15526503],\n",
       "       [0.102155  ],\n",
       "       [0.12366749],\n",
       "       [0.18844853],\n",
       "       [0.03795981],\n",
       "       [0.05253015],\n",
       "       [0.0618135 ],\n",
       "       [0.13620308],\n",
       "       [0.05151949],\n",
       "       [0.12216649],\n",
       "       [0.04716399],\n",
       "       [0.0560073 ],\n",
       "       [0.11272719],\n",
       "       [0.3114413 ],\n",
       "       [0.07043011],\n",
       "       [0.05667132],\n",
       "       [0.08686996],\n",
       "       [0.05397802],\n",
       "       [0.5068635 ],\n",
       "       [0.11938526],\n",
       "       [0.14327863],\n",
       "       [0.13273424],\n",
       "       [0.0697026 ],\n",
       "       [0.4378523 ],\n",
       "       [0.12755555],\n",
       "       [0.08152705],\n",
       "       [0.07802944],\n",
       "       [0.20732224],\n",
       "       [0.06462895],\n",
       "       [0.5215317 ],\n",
       "       [0.19187364],\n",
       "       [0.13935596],\n",
       "       [0.45331082],\n",
       "       [0.0605051 ],\n",
       "       [0.13183782],\n",
       "       [0.05476771],\n",
       "       [0.06167341],\n",
       "       [0.2762695 ],\n",
       "       [0.55354804],\n",
       "       [0.09107164],\n",
       "       [0.10115828],\n",
       "       [0.07574931],\n",
       "       [0.09136155],\n",
       "       [0.06896738],\n",
       "       [0.0986286 ],\n",
       "       [0.10703769],\n",
       "       [0.09749786],\n",
       "       [0.09511755],\n",
       "       [0.07725588],\n",
       "       [0.0653272 ],\n",
       "       [0.15026237],\n",
       "       [0.19497849],\n",
       "       [0.11534589],\n",
       "       [0.04501143],\n",
       "       [0.07541288],\n",
       "       [0.12208452],\n",
       "       [0.41407254],\n",
       "       [0.1116278 ],\n",
       "       [0.06802663],\n",
       "       [0.43869215],\n",
       "       [0.1475065 ],\n",
       "       [0.0504053 ],\n",
       "       [0.2071188 ],\n",
       "       [0.31051397],\n",
       "       [0.105524  ],\n",
       "       [0.09731141],\n",
       "       [0.05467326],\n",
       "       [0.04364018],\n",
       "       [0.07911911],\n",
       "       [0.14582668],\n",
       "       [0.1184252 ],\n",
       "       [0.16234812],\n",
       "       [0.05395495],\n",
       "       [0.08275911],\n",
       "       [0.20095573],\n",
       "       [0.4102401 ],\n",
       "       [0.09583814],\n",
       "       [0.04951936],\n",
       "       [0.18169983],\n",
       "       [0.12275274],\n",
       "       [0.42198935]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fe9a52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2f9a5e505e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(predictions)), predictions, color='blue', label='Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14284ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if prediction is > 0.2 then it is 1 else 0\n",
    "predictions = [1 if pred > 0.2 else 0 for pred in predictions]\n",
    "\n",
    "# now put it in the csv file\n",
    "submission_df = pd.DataFrame({\n",
    "    'age_group': predictions\n",
    "})\n",
    "submission_df.to_csv('submission_by_dnn_06.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127cedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
